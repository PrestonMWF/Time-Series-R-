---
title: "Forecasting I80E Traffic"
author: "Mark Preston"
date: "October 18, 2018"
output: 
  html_document: 
    fig_height: 6.5
    fig_width: 10.5
---

***

##Using Illinois Traffic data to Forecast I80E Volumes

***

###Introduction: Loading data and introducing the analysis

This week, I'll be forecasting traffic volume snumbers for I80E. For reference, this is a specific highway exit in Illinois. One of the defining features of this setup is putting together numerous data sets. The assignment includes 16 seperate .xls files, each of which represents an individual day of traffic measurements. The values, noted simply as I80E in the set, span an entire day. This means that each day has 24 different traffic volume measurements.

Basically, I've created a list of files and then converted them to data frames using a combination of `map` and `read_xls`. However these are still part of a list, so following that I've converted them to one consistent data frame with all the measurements using `map_df`. Concurrently, I've made a consistent label for date and time, filtered out some missing values, reorderd the observations (given the original counting was from 1am to 11pm, then 12am), and added the day of the week as well. The final set has three columns.

```{r loading data, warning=FALSE, message=FALSE}
library(tidyverse)
library(readxl)
library(forecast)
library(tseries)
library(lmtest)
library(knitr)
library(kableExtra)

theme_set(
  theme_minimal()
)

custom_kable <- function(x){
  kable(x, format = "html") %>%
    kable_styling(bootstrap_options = "striped")
}

traffic_files <- list.files(pattern = "*.xls")

traffic_data <- traffic_files %>%
  map(., function(x) read_xls(x, skip = 2))

dates <- c(as.Date("2013/07/01"),
           seq.Date(from = as.Date("2013/06/16"), 
                    to = as.Date("2013/06/30"), by = "day"))

day_time <- seq(from = as.POSIXct("2013-06-16 00:00"), 
                to = as.POSIXct("2013-07-01 23:00"), by = "hour")

highway_traffic <- map_df(1:16, function(x) as.data.frame(traffic_data[x])) %>%
  select(Time, I80E) %>%
  slice(-1:-2) %>%
  mutate(string_filter = str_match(string = Time, pattern = "[0-9]{2}\\:[0-9]{2}")) %>%
  filter(!is.na(string_filter)) %>%
  mutate(Day = rep(dates, each = 24)) %>%
  arrange(Day) %>%
  mutate(I80E = as.numeric(I80E),
         Day_time = day_time,
         Day_of_week = strftime(Day_time,'%A'),
         Day_of_week = factor(Day_of_week, levels =  c("Sunday", "Monday", "Tuesday", 
                                                       "Wednesday", "Thursday", 
                                                       "Friday", "Saturday"))) %>%
  select(Day, Day_of_week, I80E)

highway_traffic %>%
  head() %>%
  custom_kable()
```

***

###Exploratory data analysis: How does I80E traffic volume look over time?

With the initial data set ready, I'm starting by doing a quick summary to check on the values for each column. The date range here is from June 16 to July 1, which will be used as the forecast date. It looks like the days of the weeks are mostly even, with the exception of an extra Sunday (as a note July 1 is a Monday). One thing that sticks out here is zero for one of the measurements, which possibly represents some road closure.

```{r summary for set}
summary(highway_traffic) %>% 
  custom_kable()
```

Following the initial summary, I've developed a time series plot for the data. To add some clarity, I've coloured the split between weekdays and weekends. Using this, it seems like they have different volume patterns. However, it looks like each weekday is fairly similar, as are the weekend days.

```{r visualizing traffic time series}
highway_traffic %>%
  mutate(Day_time = day_time,
         group = "group",
         Week_Period = ifelse(Day_of_week %in% c("Sunday", "Saturday"), 
                              "Weekend", "Weekday")) %>%
  ggplot(aes(Day_time, I80E, colour = Week_Period, group = group)) +
  geom_line(size = 1.3) +
  scale_colour_manual(values = c("dodgerblue2", "darkorange")) +
  labs(title = "Traffic on I80E from June 16 to July 1 (coloured by weekend/weekday split)",
       subtitle = "Travel volumes look different for weekday and weekend")
```

To check stationarity, I'm doing an ADF test. The low p-value signals that the series is not stationary given the null hypothesis here is non-stationary.

```{r kpss test, warning=FALSE}
adf.test(highway_traffic$I80E)
```

With the statonarity checked, I wanted to isolate how one sample for each day of the week appears. As the previous plot suggests, Monday through Friday have similar traffic patterns which are distinctive from Saturday and Sunday.

```{r isolated day of week vis}
highway_traffic %>%
  slice(1:168) %>%
  mutate(hour = rep(1:24, 7)) %>%
  ggplot(aes(hour, I80E, colour = Day_of_week)) +
  geom_line(size = 1.3, show.legend = F) +
  facet_wrap(facets = "Day_of_week") +
  scale_x_continuous(breaks = seq(1, 24, 2)) +
  labs(title = "Traffic on I80E by day of week from June 16 to 22",
       subtitle = "Weekdays and weekends have different travel patterns likely owing to work commuting")
```

***

###ARIMA Modelling

With the initial exploration done, I'll move onto the modelling phase. To start, ARIMA models require a `ts` object so I'm converting the original data to this format. I've picked a frequency of 24, which corresponds to the number of measurements in each day. To check if I did this right, I've visualized the time series using the `autoplot` function. As seen, it seems to match the initial plotting I did so I can move forward with the object.

```{r modelling set up}
hourly_traffic <- highway_traffic %>%
  ts(frequency = 24)

autoplot(hourly_traffic[,"I80E"]) +
  geom_line(size = 1.3, colour = "dodgerblue2") +
  labs(title = "Traffic on I80E from June 16 to July 1- TS object plot matches original data",
       y = "I80E")
```

Following this, I'm splitting the series into a train and test set. As aforementioned, I'll be forecasting traffic for July 1.

```{r training and test split}
hourly_train <- window(hourly_traffic, end = 15.95833)

hourly_test <- window(hourly_traffic, start = 16)
```

####Part 1 - Use ARIMA(p,d,q) model to forecast. Find the model returned by auto.arima()

I've started by developing a non-seasonal ARIMA model using the `auto.arima` function, which works to find the best model for p, d, and q.

- Arima value P (AR): Indicating 2 autoregressive terms 

- Arima value D (d): 0, which is expected given the significant ADF test

- Arima value (MA): Indicating 3 lagges forecast errors

The summary also shows the series is non-zero mean. In this case, I think means that there is seasonality present (leading to statistical equillibrium being violated). As a methodological note, I've turned that option off to start as per the question.

Aside from the coefficients, the goodness of fit and model quality metrics are important to comment on. This model has an AICc of 4443.83, which by itself offers no insight because AICc is a measure of relative fit so I'll need to compare it to other models to verify the value. Auto arima is generally supposed to return a very good model but, I'll develop some of my own using `Arima` to see if I can get a better AICc. During this process, I'll review BIC at the same time.

```{r auto arima model dev, cache=TRUE}
(hourly_arima <- auto.arima(y = hourly_train[,"I80E"], 
                           seasonal = F, 
                           stepwise = F, 
                           approximation = F))
```

Before moving on to the model comparisons, I wanted to check the residuals. The histogram shows normal residuals but, the ACF, residual time series, and the Ljung-Box test all signal residual autocorrelation. Again, this is typical of seasonality not being removed so I think it makes sense to use a seasonal model and see how the model fit is.

```{r residuals check}
checkresiduals(hourly_arima)
```

####Change the values of p and q and determine the best model using AICc and BIC. Do AICc and BIC select the same model as the best model?

To better understand how differing values of p and q affect the arima AICc and BIC, I've developed a testing method to review different combinations of each value. To do so, I used `cross2` in conjunction with `map` to try all combinations of values where p = 1-4 and q = 1-7. This renders 28 different permutations (4 * 7), each of which comprises a different arima model. I decided to leave the d = 0 as per the previous `auto.arima` model.

I chose the top then AICc and BIC p and q combinations from the 28 possible choices, which are displayed together in the table below. Of note, there are 9 models with a higher AICc than the p = 2 and q = 3 model rendered by `auto.arima` (it places tenth). On the BIC side, 2 & 3 isn't present. The top two models from both AICc and BIC are common, though they have a different order depending on side. It seems that the metrics disagree on the top spot but, they are broadly similar which is good to see. P = 3 and q = 5 is the top AIC with 4470.327, a 29 point difference from the initial model, while the p = 3 and q = 4 tops the BIC side.

```{r manual arima dev, cache=TRUE}
aicc_compare <- cross2(1:4, 1:7) %>%
  map(~Arima(y =  hourly_train[,"I80E"],  
             order = c(.x[[1]], 0, .x[[2]]))$aicc) %>%
  data.frame() %>%
  t() %>%
  as.data.frame() %>%
  mutate(AICc_combination = paste(rep(1:4, 7), "&", rep(1:7, each = 4))) %>%
  rename(AICc = V1) %>%
  select(AICc_combination, AICc) %>%
  arrange(AICc) %>%
  slice(1:10)

bic_compare <- cross2(1:4, 1:7) %>%
  map(~Arima(y =  hourly_train[,"I80E"],  
             order = c(.x[[1]], 0, .x[[2]]))$bic) %>%
  data.frame() %>%
  t() %>%
  as.data.frame() %>%
  mutate(BIC_combination = paste(rep(1:4, 7), "&", rep(1:7, each = 4))) %>%
  rename(BIC = V1) %>%
  select(BIC_combination, BIC) %>%
  arrange(BIC) %>%
  slice(1:10)

bind_cols(aicc_compare, bic_compare) %>% 
  custom_kable()
```

With this new insight, I've developed the p = 3 and q = 5 model to review it further. As seen, it has the same AICc but different coefficients for AR and MA owing to a new model composition.

```{r manual arima dev with new aic model, cache=TRUE}
hourly_high_aic <- Arima(y =  hourly_train[,"I80E"], order = c(3, 0, 5))

summary(hourly_high_aic)
```

I wanted to check the significance of the added coefficients here, which can be done using `coeftest`. Only MA 5 is significant amongst the new moving average terms while AR 3 is.

```{r coef testing for high aic model}
coeftest(hourly_high_aic)
```

That said, I still need to see how the models forecast in test. Below, I've run the forecast for both models and plotted the results. Neither model seems to perform well on test given the seasonality omission. The same broad upward trend is captured but,the inherent time of day and day of week variation is missed.

```{r forecasting for test}
high_aic_forecast <- forecast(hourly_high_aic, h = 24)

nonseasonal_forecast <- forecast(hourly_arima, h = 24)

autoplot(hourly_traffic[,"I80E"]) +
  autolayer(high_aic_forecast, series = "High_AICc", PI = FALSE) +
  autolayer(nonseasonal_forecast, series = "Auto_arima", PI = FALSE) +
  geom_line(colour = "darkgray") +
  labs(title = "Forecast for High AICc ARIMA model- Model does not pick up seasonality",
       y = "Traffic Volume") +
  guides(colour = guide_legend(title = "Forecast"))
```

Between both models though, the manually derived arima shows a lower sum of squared errors (SSE) signalling a better fit.

```{r sum of sq error review}
c(high_aic_SSE = sum(high_aic_forecast$residuals ^ 2),
  auto_SSE = sum(nonseasonal_forecast$residuals ^ 2)) %>%
  custom_kable()
```


####Part 2 - Use day of the week seasonal ARIMA(p,d,q)(P,Q,D) to forecast for July 1

To create this time series forecast, I'll need to recast the original data with a new frequncy. For this, I'm doing 24 * 7 (hours times days in week) to get the new object. As seen in the output plot, the series pattern remains consistent signalling the development should be okay.

```{r daily time series data creation}
day_traffic <- highway_traffic %>%
  ts(frequency = 24 * 7)

day_train <- window(day_traffic, end = 3.136905)

day_test <- window(day_traffic, start = 3.142857)

autoplot(day_traffic[,"I80E"]) +
  geom_line(size = 1.3, colour = "dodgerblue2") +
  labs(title = "Traffic on I80E from June 16 to July 1- TS object plot matches original data",
       subtitle = "Frequency here is 24 * 7 instead of original 24",
       y = "I80E")
```

After developing the new model, I feel like this frequency should yield a better forecast. The AIC is substantailly lower than the previous non-seasonal arima.

```{r seasonal model devleopment for daily, cache=TRUE, error=FALSE, warning=FALSE}
(daily_seasonal <- auto.arima(y =  day_train[,"I80E"], 
                              approximation = F, 
                              stepwise = F))
```

I've done the residual check and the series looks stationary when using 100 lags for the Ljung-Box test. With lags over 120, the p-value is slightly under .05 but, I'm satisfied that the series has statistical equillibrium.  

```{r daily redisual check}
checkresiduals(daily_seasonal, lag = 100)
```

The actual forecast shows a very close fit to the July 1 values as well.

```{r forecasting for test- daily model}
daily_forecast <- forecast(daily_seasonal, h = 24)

autoplot(day_traffic[,"I80E"]) +
  autolayer(daily_forecast, series = "Daily_arima", PI = FALSE) +
  geom_line(colour = "darkgray") +
  labs(title = "Forecast for Daily seasonal ARIMA model",
       subtitle = "Fit looks to follow July values better than non-seasonal hourly",
       y = "Traffic Volume") +
  guides(colour = guide_legend(title = "Forecast"))
```

To confirm this though, I've created accuracy tables for the model. There are some Inf values owing to a negative prediction in the training set but, the test shows good results. These would have to be confirmed with any business goal with the department but, an 11% mean average percentage error (MAPE) seems close to me.

```{r test forecast accuracy auto arima daily}
accuracy(daily_forecast, day_test[,"I80E"]) %>%
  custom_kable()
```


####Part 3 - Use hour of the day seasonal ARIMA (p,d,q)(P,D,Q) model to forecast for the hours 8:00, 9:00, 17:00 and 18:00 on July 1

I've developed a seasonal arima model with `auto.arima`. The summary shows a large improvement from the non-seasonal model when asessing AIC. The differencing value (D = 1) also shows that the model incorporates elements to make the series zero mean (i.e. stationary). However, it still seems to be a worse fit than the daily model.

```{r seasonal model devleopment, cache=TRUE}
(hourly_seasonal <- auto.arima(y =  hourly_train[,"I80E"], 
                              seasonal = T,
                              approximation = F, 
                              stepwise = F))
```

a coefficient review for the model shows that all the terms are significant.

```{r hourly seasonal coef review}
coeftest(hourly_seasonal)
```

Moving into the residuals review, it's the model seems to display stationarity. The ACF has a few spikes but, over 72 lags, some might randomly occur. The Ljung-Box test shows a high p-value further signalling stationarity.

```{r residual review for seasonal model}
checkresiduals(hourly_seasonal)
```

I've developed the July 1 forecasts and plotted the results. It appears that the `auto.arima` model has a reasonably good fit but, not on par with the daily model.

```{r forecasting for test- seasonal model}
hourly_forecast <- forecast(hourly_seasonal, h = 24)

autoplot(hourly_traffic[,"I80E"]) +
  autolayer(hourly_forecast, series = "Hourly_arima", PI = FALSE) +
  geom_line(colour = "darkgray") +
  labs(title = "Forecast for High AICc ARIMA model- Model does not pick up seasonality",
       y = "Traffic Volume") +
  guides(colour = guide_legend(title = "Forecast"))
```

As always though, I've created accuracy tables to review the model. The MAPE is much higher (21% vs 11%) so it would appear the daily model is preferable.

```{r test forecast accuracy auto arima seasonal}
accuracy(hourly_forecast, hourly_test[,"I80E"]) %>%
  custom_kable()
```

####Part 4 - For the July 1 8:00, 9:00, 17:00 and 18:00 forecasts, which model is better (part 2 or part 3)?

Before looking at the final accuracy comparison, I wanted to visualize the main models from each part in one spot. The daily fit would be nearly identical but, the missing zero at 19:00 throws off the forecast. It might be appropriate to impute a value here given how much it affects the accuracy here. Working from top to bottom, it's possible to see the forecast shape improving as each method becomes slightly more nuanced.

```{r all forecast vis for test, warning=FALSE}
highway_traffic %>%
  select(I80E) %>%
  slice(361:384) %>%
  mutate(Time = seq(1, n(), 1),
         Daily_fit = daily_forecast$mean,
         Hourly_fit = hourly_forecast$mean,
         Nonseasonal_aic = high_aic_forecast$mean) %>%
  gather(key = "Model", value = "Value", -Time) %>%
  mutate(Model = factor(Model, levels = c("I80E", "Daily_fit", 
                                          "Hourly_fit", "Nonseasonal_aic"))) %>%
  ggplot(aes(Time, Value, colour = Model)) +
  geom_line(size = 2) +
  facet_wrap(facets = "Model", ncol = 1) +
  scale_x_continuous(breaks = seq(0, 24, 2)) +
  scale_colour_manual(values = c("dodgerblue2", "darkorange", 
                                 "darkorchid", "hotpink3")) +
labs(title = "Fitted values for each ARIMA forecast and actuals (I80E)- Model fiited values seem close",
     subtitle = "Auto arima model appears to produce better forecast",
       y = "Traffic Level")
```

The assignment is specifically looking for forecasts for 8am, 9am, 5pm (17:00), and 6pm (18:00). I've gathered those results for the daily and hourly seasonal models below for comparison. Not surprsingly following the model review, the daily forecast is much better. This was evident throughout when assessing the visual aspects of the model and goodness of fit metrics like AIC. The table shows the percent misses on these forcassts is substantially lower for the daily model.

```{r gathering needed time forecasts, message=FALSE, warning=FALSE}
results_comparision <- as.data.frame(daily_forecast$mean) %>%
  mutate(Set = "Seasonal_daily",
         I80E = as.numeric(hourly_test[,"I80E"]),
         Difference = as.numeric(I80E - x),
         Percent_miss = round(Difference / I80E, 4) * 100,
         Time = 1:24) %>%
  filter(Time %in% c(8, 9, 17, 18)) %>%
  rename(Forecast = x) %>%
  select(Set, Time, I80E, Forecast, Difference, Percent_miss)

as.data.frame(hourly_forecast$mean) %>%
  mutate(Set = "Seasonal_hourly",
         I80E = as.numeric(hourly_test[,"I80E"]),
         Difference = I80E - x,
         Percent_miss = round(Difference / I80E, 4) * 100,
         Time = 1:24) %>%
  filter(Time %in% c(8, 9, 17, 18)) %>%
  rename(Forecast = x) %>%
  select(Set, Time, I80E, Forecast, Difference, Percent_miss) %>%
  bind_rows(results_comparision) %>%
  custom_kable()
```

As a final point, I've also included the SSE where the daily model is almost three times less than the hourly model. 

```{r final SSE review}
c(Daily_SSE = sum(daily_forecast$residuals ^ 2),
  Hourly_SSE = sum(hourly_forecast$residuals ^ 2)) %>%
  custom_kable()
```

***
